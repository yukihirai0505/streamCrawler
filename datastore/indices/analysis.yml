analysis:
  tokenizer:
    # 日本語用の３グラム分割トークナイザー
    ngram_tokenizer:
      type: nGram
      min_gram: 2
      max_gram: 3
      token_chars: [letter, digit]
    # kuromojiでの形態素解析によるトークナイザー
    ja_tokenizer:
      type: kuromoji_tokenizer
      mode: search
  analyzer:
    # 日本語形態素解析
    ja_analyzer:
      alias: [japanese,ja]
      type: custom
      # htmlタグを取り除き、大文字小文字を正規化
      char_filter: [html_strip, icu_normalizer]
      tokenizer: ja_tokenizer
      filter: [kuromoji_stemmer, kuromoji_part_of_speech, kuromoji_baseform]
    # 日本語バイグラム
    ngram_analyzer:
      type: custom
      char_filter: [html_strip]
      tokenizer: ngram_tokenizer
      filter: [cjk_width, lowercase]
